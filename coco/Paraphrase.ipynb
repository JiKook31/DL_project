{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtOtpV3JLBxK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Conda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Conda\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0A2ePOkSdI_l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self, vocab_size, residual=True):\n",
    "        self.residual = residual\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def make_graph(self,mode, features, labels):\n",
    "        embed_dim = 256\n",
    "        num_units = 256\n",
    "\n",
    "        input,output   = features['input'], features['output']\n",
    "        batch_size     = tf.shape(input)[0]\n",
    "        start_tokens   = tf.zeros([batch_size], dtype= tf.int64)\n",
    "        train_output   = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "        input_lengths  = tf.reduce_sum(tf.to_int32(tf.not_equal(input, 1)), 1)\n",
    "        output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "        input_embed    = layers.embed_sequence(input, vocab_size=self.vocab_size, embed_dim = embed_dim, scope = 'embed')\n",
    "        output_embed   = layers.embed_sequence(train_output, vocab_size=self.vocab_size, embed_dim = embed_dim, scope = 'embed', reuse = True)\n",
    "        with tf.variable_scope('embed', reuse=True):\n",
    "            embeddings = tf.get_variable('embeddings')\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n",
    "        if self.residual:\n",
    "            cell = tf.contrib.rnn.ResidualWrapper(cell)\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell, input_embed, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            # Decoder is partially based on @ilblackdragon//tf_example/seq2seq.py\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    num_units=num_units, memory=encoder_outputs,\n",
    "                    memory_sequence_length=input_lengths)\n",
    "                cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n",
    "                attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "                out_cell = tf.contrib.rnn.OutputProjectionWrapper(attn_cell, self.vocab_size, reuse=reuse)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=out_cell, helper=helper,\n",
    "                    initial_state=out_cell.zero_state(\n",
    "                        dtype=tf.float32, batch_size=batch_size))\n",
    "                outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder=decoder, output_time_major=False,\n",
    "                    impute_finished=True, maximum_iterations=30)\n",
    "                return outputs[0]\n",
    "\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "        pred_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens=tf.to_int32(start_tokens), end_token=1)\n",
    "        train_outputs = decode(train_helper, 'decode')\n",
    "        pred_outputs  = decode(pred_helper, 'decode', reuse=True)\n",
    "\n",
    "        tf.identity(train_outputs.sample_id[0], name='train_pred')\n",
    "        weights = tf.to_float(tf.not_equal(train_output[:, :-1], 1))\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(train_outputs.rnn_output, output, weights=weights)\n",
    "        train_op = layers.optimize_loss(\n",
    "            loss, tf.train.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=0.001,\n",
    "            summaries=['loss', 'learning_rate'])\n",
    "\n",
    "        tf.identity(pred_outputs.sample_id[0], name='predict')\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=pred_outputs.sample_id, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, input_filename, output_filename, vocab_filename):\n",
    "        self.input_filename = input_filename\n",
    "        self.output_filename = output_filename\n",
    "        self.vocab_filename = vocab_filename\n",
    "        \n",
    "        # create vocab and reverse vocab maps\n",
    "        self.vocab     = {}\n",
    "        self.rev_vocab = {}\n",
    "        self.END_TOKEN = 1 \n",
    "        self.UNK_TOKEN = 2\n",
    "        self.FLIP = False\n",
    "        with open(vocab_filename) as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                self.vocab[line.strip()] = idx\n",
    "                self.rev_vocab[idx] = line.strip()\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def tokenize_and_map(self,line):\n",
    "        return [self.vocab.get(token, self.UNK_TOKEN) for token in line.split(' ')]\n",
    "\n",
    "    def prepare(self,text):\n",
    "        tokens = self.tokenize_and_map(text)\n",
    "        input_length   = len(tokens)\n",
    "        source = [tokens]\n",
    "        source[0] += [self.END_TOKEN] * (input_length - len(source[0]))\n",
    "        return source\n",
    "\n",
    "\n",
    "\n",
    "    def single(self, sentence):\n",
    "        tokens = self.tokenize_and_map(sentence)\n",
    "        def input_fn():\n",
    "            inp = tf.placeholder(tf.int64, shape=[None, None], name='input')\n",
    "            output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "            tf.identity(inp[0], 'source')\n",
    "            tf.identity(output[0], 'target')\n",
    "            return { 'input': inp, 'output': output}, None\n",
    "        def feed_fn():\n",
    "            input_length   = len(tokens)\n",
    "            source = [tokens]\n",
    "            source[0] += [self.END_TOKEN] * (input_length - len(source[0]))\n",
    "            # this source is not used to compute anything, just so that placeholder does not complain about\n",
    "            # missing values for target during prediction\n",
    "            self.FLIP = not self.FLIP\n",
    "            if not self.FLIP:\n",
    "                raise StopIteration\n",
    "\n",
    "            return { 'input:0': source, 'output:0': source }\n",
    "        return input_fn, feed_fn\n",
    "\n",
    "    def make_input_fn(self):\n",
    "        def input_fn():\n",
    "            inp = tf.placeholder(tf.int64, shape=[None, None], name='input')\n",
    "            output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "            tf.identity(inp[0], 'source')\n",
    "            tf.identity(output[0], 'target')\n",
    "            return { 'input': inp, 'output': output}, None\n",
    "\n",
    "        def sampler():\n",
    "            while True:\n",
    "                with open(self.input_filename) as finput, open(self.output_filename) as foutput:\n",
    "                    for source,target in zip(finput, foutput):\n",
    "                        yield {\n",
    "                            'input': self.tokenize_and_map(source)[:30 - 1] + [self.END_TOKEN],\n",
    "                            'output': self.tokenize_and_map(target)[:30 - 1] + [self.END_TOKEN]}\n",
    "\n",
    "        data_feed = sampler()\n",
    "        def feed_fn():\n",
    "            source, target = [], []\n",
    "            input_length, output_length = 0, 0\n",
    "            for i in range(32):\n",
    "                rec = data_feed.__next__()\n",
    "                source.append(rec['input'])\n",
    "                target.append(rec['output'])\n",
    "                input_length = max(input_length, len(source[-1]))\n",
    "                output_length = max(output_length, len(target[-1]))\n",
    "            for i in range(32):\n",
    "                source[i] += [self.END_TOKEN] * (input_length - len(source[i]))\n",
    "                target[i] += [self.END_TOKEN] * (output_length - len(target[i]))\n",
    "            return { 'input:0': source, 'output:0': target }\n",
    "        return input_fn, feed_fn\n",
    "\n",
    "    def get_formatter(self,keys):\n",
    "        def to_str(sequence):\n",
    "            tokens = [\n",
    "                self.rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "        def format(values):\n",
    "            res = []\n",
    "            for key in keys:\n",
    "                res.append(\"****%s == %s\" % (key, to_str(values[key]).replace('</S>','').replace('<S>', '')))\n",
    "            return '\\n'+'\\n'.join(res)\n",
    "        return format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    def __init__(self, checkpoint='checkpoint'):\n",
    "        self.data  = Data('train_source.txt', 'train_target.txt', 'train_vocab.txt')\n",
    "        model = Seq2seq(self.data.vocab_size)\n",
    "        estimator = tf.estimator.Estimator(model_fn=model.make_graph, model_dir=checkpoint)\n",
    "        def input_fn():\n",
    "            inp = tf.placeholder(tf.int64, shape=[None, None], name='input')\n",
    "            output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "            tf.identity(inp[0], 'source')\n",
    "            tf.identity(output[0], 'target')\n",
    "            dict =  { 'input': inp, 'output': output}\n",
    "            return tf.estimator.export.ServingInputReceiver(dict, dict)\n",
    "        self.predictor = tf.contrib.predictor.from_estimator(estimator, input_fn)\n",
    "\n",
    "    def infer(self, sentence):\n",
    "        input = self.data.prepare(sentence)\n",
    "        predictor_prediction = self.predictor({\"input\": input, \"output\":input})\n",
    "        print(predictor_prediction)\n",
    "        words = [self.data.rev_vocab.get(i, '<UNK>') for i in predictor_prediction['output'][0] if i > 2]\n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'checkpoint', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000209B1773160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "P = Predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': array([[  533,  4897,  4897,  4897,  4897,  4897,  4897,  4897,  4897,\n",
      "         4897,  4897,  1130,  1130,  1130, 16004, 16004, 16004,  9812,\n",
      "        12323, 17747, 17747, 18364,  6247,  6247,  6247,  6247, 15452,\n",
      "         6247, 15452,  6247]])}\n"
     ]
    }
   ],
   "source": [
    "res = P.infer('what be the symbol of magnesium sulphate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weighing erect erect erect squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl squirrl hooking hooking hooking hooking hooking eye squirrl squirrl weighing \"troop \"troop furinture'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Paraphrase.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
